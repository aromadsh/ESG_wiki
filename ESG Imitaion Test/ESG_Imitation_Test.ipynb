{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 사용할 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, process_pdf\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from io import StringIO\n",
    "from io import open\n",
    "from urllib.request import urlopen\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PDF 파일 읽어오기\n",
    "\n",
    "- ESG의 대한 기업의 대한 정보를 가져오기 위해서 주로 PDF의 파일로 된 지속가능경영보고서를 읽음\n",
    "\n",
    "- 중소기업의 지속가능경영보고서 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_file(pdfFile):\n",
    "    pdfrm = PDFResourceManager()\n",
    "    strio = StringIO()\n",
    "    lapa = LAParams()\n",
    "    device = TextConverter(pdfrm, strio, laparams = lapa)\n",
    "    \n",
    "    process_pdf(pdfrm, device, pdfFile)\n",
    "    device.close()\n",
    "    \n",
    "    content = strio.getvalue()\n",
    "    strio.close()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Cannot locate objid=521\n",
      "WARNING:pdfminer.layout:Too many boxes (167) to group, skipping.\n",
      "WARNING:root:Cannot locate objid=536\n",
      "WARNING:root:Cannot locate objid=612\n",
      "WARNING:root:Cannot locate objid=588\n",
      "WARNING:root:Cannot locate objid=505\n",
      "WARNING:pdfminer.layout:Too many boxes (104) to group, skipping.\n"
     ]
    }
   ],
   "source": [
    "pdf_a = open(\"data/ESG_Mid/중소기업.pdf\", \"rb\")\n",
    "a = read_pdf_file(pdf_a)\n",
    "pdf_a.close() \n",
    "\n",
    "pdf_b = open(\"data/ESG_Mid/중소기업.pdf\", \"rb\")\n",
    "b = read_pdf_file(pdf_b)\n",
    "pdf_b.close()\n",
    "\n",
    "pdf_c = open(\"data/ESG_Mid/중소기업.pdf\", \"rb\")\n",
    "c = read_pdf_file(pdf_c)\n",
    "pdf_c.close()\n",
    "\n",
    "pdf_d = open(\"data/ESG_Mid/중소기업.pdf\", \"rb\")\n",
    "d = read_pdf_file(pdf_d)\n",
    "pdf_d.close()\n",
    "\n",
    "pdf_e = open(\"data/ESG_Mid/중소기업.pdf\", \"rb\")\n",
    "e = read_pdf_file(pdf_e)\n",
    "pdf_e.close()\n",
    "\n",
    "pdf_f = open(\"data/ESG_Mid/중소기업.pdf\", \"rb\")\n",
    "f = read_pdf_file(pdf_f)\n",
    "pdf_f.close() \n",
    "\n",
    "pdf_g = open(\"data/ESG_Mid/중소기업.pdf\", \"rb\")\n",
    "g = read_pdf_file(pdf_g)\n",
    "pdf_g.close()\n",
    "\n",
    "pdf_h = open(\"data/ESG_Mid/중소기업.pdf\", \"rb\")\n",
    "h = read_pdf_file(pdf_h)\n",
    "pdf_h.close()\n",
    "\n",
    "pdf_i = open(\"data/ESG_Mid/중소기업.pdf\", \"rb\")\n",
    "i = read_pdf_file(pdf_i)\n",
    "pdf_i.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 문서 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "def word_token (x) :\n",
    "    tokens = []\n",
    "    for token in mecab.pos(x):\n",
    "        tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "a = word_token(a)\n",
    "b = word_token(b)\n",
    "c = word_token(c)\n",
    "d = word_token(d)\n",
    "e = word_token(e)\n",
    "f = word_token(f)\n",
    "g = word_token(g)\n",
    "h = word_token(h)\n",
    "i = word_token(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 불용어 처리 및 TF-IDF\n",
    "\n",
    "- stop 리스트는 불용어를 넣어 처리하는 부분이며, 임의적으로 변경이 가능합니다. 이부분은 각 문서를 토큰화하여 불용어를 찾아 리스트에 추가해도 되었지만 TF-IDF의 경우 빈도 수치가 높은 단어를 가져오기 때문에 불용어를 따로 처리할 필요가 없었습니다.\n",
    "\n",
    "- feature 값과 label 값을 나누기 위해서 W0, w1, w2 ... w14와 v0, v1, v2 ... v14 로 나누어 저장하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "stop = [\"회사\", \"에서\", \"위해\", \"관련\", \"기준\"]\n",
    "\n",
    "def vocab_nodes (x):\n",
    "    nodes = [t[0] for t in x]\n",
    "    vocab = [t[0] for t in x if t[0] not in stop if t[1] in ['NNG', 'NNP'] and len(t[0]) > 1]\n",
    "    \n",
    "    vocab = list(set(vocab))\n",
    "\n",
    "    vocab2idx = {vocab[i]:i for i in range(len(vocab))}\n",
    "    idx2vocab = {i:vocab[i] for i in range(len(vocab))}\n",
    "    \n",
    "    vocab_len = len(vocab2idx)\n",
    "\n",
    "    # 토큰별로 그래프 edge를 Matrix 형태로 생성\n",
    "    weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
    "\n",
    "    # 각 토큰 노드별로 스코어 1로 초기화\n",
    "    score = np.ones((vocab_len),dtype=np.float32)\n",
    "\n",
    "    # coocurrence를 판단하기 위한 window 사이즈 설정\n",
    "    window_size = 4\n",
    "    covered_coocurrences = []\n",
    "\n",
    "    for window_start in range(len(nodes) - window_size + 1):\n",
    "        window = nodes[window_start:window_start+window_size]\n",
    "        for i in range(window_size):\n",
    "            for j in range(i+1, window_size):\n",
    "                if window[i] in vocab and window[j] in vocab:\n",
    "                    index_i = window_start + i\n",
    "                    index_j = window_start + j\n",
    "\n",
    "                    if (index_i, index_j) not in covered_coocurrences:\n",
    "                        weighted_edge[vocab2idx[window[i]]][vocab2idx[window[j]]] = 1\n",
    "                        weighted_edge[vocab2idx[window[j]]][vocab2idx[window[i]]] = 1\n",
    "                        covered_coocurrences.append((index_i, index_j))\n",
    "\n",
    "    for i in range(vocab_len):\n",
    "        row_sum = weighted_edge[i].sum()\n",
    "        weighted_edge[i] = weighted_edge[i]/row_sum if row_sum > 0 else 0\n",
    "\n",
    "    MAX_ITERATIONS = 50\n",
    "    d=0.85\n",
    "    threshold = 0.0001 #convergence threshold\n",
    "\n",
    "    for iter in range(MAX_ITERATIONS):\n",
    "        prev_score = np.copy(score)\n",
    "\n",
    "        for i in range(vocab_len):\n",
    "            summation = 0\n",
    "            for j in range(vocab_len):\n",
    "                if weighted_edge[j][i] != 0:\n",
    "                    summation += weighted_edge[j][i] * prev_score[j]\n",
    "\n",
    "            score[i] = (1 - d) * d*summation\n",
    "\n",
    "        if np.sum(np.fabs(prev_score -  score)) <= threshold:\n",
    "            break\n",
    "\n",
    "\n",
    "    sorted_index = np.flip(np.argsort(score), 0)\n",
    "\n",
    "    w = []\n",
    "    v = []\n",
    "    for i in range(0, 100) :\n",
    "        w.append(str(idx2vocab[sorted_index[i]]))\n",
    "        v.append(str(score[sorted_index[i]]))\n",
    "    return w, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0, v0 = vocab_nodes (a)\n",
    "w1, v1 = vocab_nodes (b)\n",
    "w2, v2 = vocab_nodes (c)\n",
    "w3, v3 = vocab_nodes (d)\n",
    "w4, v4 = vocab_nodes (e)\n",
    "w5, v5 = vocab_nodes (f)\n",
    "w6, v6 = vocab_nodes (g)\n",
    "w7, v7 = vocab_nodes (h)\n",
    "w8, v8 = vocab_nodes (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 데이터 프레임 생성 및 CSV 수정\n",
    "\n",
    "- 단어의 값을 시리즈로 만들어 데이터 프레임으로 만들어서 feature 값의 모음을 만들었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = []\n",
    "for i in range(1, 101):\n",
    "    attr.append(f\"속성{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = pd.Series(w0, attr)\n",
    "w1 = pd.Series(w1, attr)\n",
    "w2 = pd.Series(w2, attr)\n",
    "w3 = pd.Series(w3, attr)\n",
    "w4 = pd.Series(w4, attr)\n",
    "w5 = pd.Series(w5, attr)\n",
    "w6 = pd.Series(w6, attr)\n",
    "w7 = pd.Series(w7, attr)\n",
    "w8 = pd.Series(w8, attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([w0, w1, w2, w3, w4, w5, w6, w7, w8], index=[1, 2, 3, 4, 5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame(index=range(0, 9), columns=[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label (x) :\n",
    "    col_list = list(df[x])\n",
    "    en = LabelEncoder()\n",
    "    en.fit(df[x])\n",
    "    x_list = en.transform(df[x])\n",
    "    \n",
    "    return x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in df:\n",
    "    a.append(label(f\"{i}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = pd.DataFrame(data = a, columns=[x for x in range(1, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = df_label.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standard = pd.read_csv(\"data/csv/label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_standard.drop([\"y\", \"Unnamed: 0\"], axis=1, inplace = False)\n",
    "df_target = df_standard[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = RandomForestRegressor()\n",
    "\n",
    "model1.fit(df_train, df_target)\n",
    "\n",
    "\n",
    "pred1 = model1.predict(df_label)\n",
    "\n",
    "df_result['result'] = pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>93.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93.254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92.996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>92.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>92.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>93.137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   result\n",
       "0  93.592\n",
       "1  93.254\n",
       "2  92.888\n",
       "3  92.996\n",
       "4  93.576\n",
       "5  92.958\n",
       "6  92.917\n",
       "7  93.288\n",
       "8  93.137"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
